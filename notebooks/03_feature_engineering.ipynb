{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3464a4da-42a7-4395-9b6e-3aa21d0428ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering & Target Creation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e49ee83-6c13-446d-93eb-2c55eb537741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "sales_df = pd.read_csv('data_cleaned/sales_clean.csv')\n",
    "inventory_df = pd.read_csv('data_cleaned/inventory_clean.csv')  \n",
    "external_df = pd.read_csv('data_cleaned/external_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a68cc05-8b8c-4e15-a756-a03b159381f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Sales records: 31696\n",
      "Inventory snapshots: 8343\n"
     ]
    }
   ],
   "source": [
    "# Convert dates\n",
    "sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "inventory_df['snapshot_date'] = pd.to_datetime(inventory_df['snapshot_date'])\n",
    "external_df['date'] = pd.to_datetime(external_df['date'])\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Sales records: {len(sales_df)}\")\n",
    "print(f\"Inventory snapshots: {len(inventory_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841fcf2-6384-4932-a311-b92713044859",
   "metadata": {},
   "source": [
    "# CREATE SALES FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bbeb126-5082-441a-90c5-52c8c84f521a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sales_quantity</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>size</th>\n",
       "      <th>city</th>\n",
       "      <th>category</th>\n",
       "      <th>unit_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN72449</td>\n",
       "      <td>ST001</td>\n",
       "      <td>P001</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>180.76</td>\n",
       "      <td>723.04</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2022</td>\n",
       "      <td>True</td>\n",
       "      <td>Downtown Grocery</td>\n",
       "      <td>grocery</td>\n",
       "      <td>medium</td>\n",
       "      <td>Boston</td>\n",
       "      <td>grocery</td>\n",
       "      <td>107.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN25921</td>\n",
       "      <td>ST001</td>\n",
       "      <td>P003</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>16</td>\n",
       "      <td>340.00</td>\n",
       "      <td>5440.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2022</td>\n",
       "      <td>True</td>\n",
       "      <td>Downtown Grocery</td>\n",
       "      <td>grocery</td>\n",
       "      <td>medium</td>\n",
       "      <td>Boston</td>\n",
       "      <td>grocery</td>\n",
       "      <td>159.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN12345</td>\n",
       "      <td>ST001</td>\n",
       "      <td>P004</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>14</td>\n",
       "      <td>239.67</td>\n",
       "      <td>3355.38</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2022</td>\n",
       "      <td>True</td>\n",
       "      <td>Downtown Grocery</td>\n",
       "      <td>grocery</td>\n",
       "      <td>medium</td>\n",
       "      <td>Boston</td>\n",
       "      <td>grocery</td>\n",
       "      <td>135.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN61515</td>\n",
       "      <td>ST001</td>\n",
       "      <td>P006</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>9</td>\n",
       "      <td>309.04</td>\n",
       "      <td>2781.36</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2022</td>\n",
       "      <td>True</td>\n",
       "      <td>Downtown Grocery</td>\n",
       "      <td>grocery</td>\n",
       "      <td>medium</td>\n",
       "      <td>Boston</td>\n",
       "      <td>grocery</td>\n",
       "      <td>151.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN98706</td>\n",
       "      <td>ST001</td>\n",
       "      <td>P009</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>12</td>\n",
       "      <td>131.46</td>\n",
       "      <td>1577.52</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2022</td>\n",
       "      <td>True</td>\n",
       "      <td>Downtown Grocery</td>\n",
       "      <td>grocery</td>\n",
       "      <td>medium</td>\n",
       "      <td>Boston</td>\n",
       "      <td>grocery</td>\n",
       "      <td>54.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  transaction_id store_id product_id       date  sales_quantity  unit_price  \\\n",
       "0       TXN72449    ST001       P001 2022-01-01               4      180.76   \n",
       "1       TXN25921    ST001       P003 2022-01-01              16      340.00   \n",
       "2       TXN12345    ST001       P004 2022-01-01              14      239.67   \n",
       "3       TXN61515    ST001       P006 2022-01-01               9      309.04   \n",
       "4       TXN98706    ST001       P009 2022-01-01              12      131.46   \n",
       "\n",
       "   total_revenue  month day_of_week  year  is_weekend              name  \\\n",
       "0         723.04      1    Saturday  2022        True  Downtown Grocery   \n",
       "1        5440.00      1    Saturday  2022        True  Downtown Grocery   \n",
       "2        3355.38      1    Saturday  2022        True  Downtown Grocery   \n",
       "3        2781.36      1    Saturday  2022        True  Downtown Grocery   \n",
       "4        1577.52      1    Saturday  2022        True  Downtown Grocery   \n",
       "\n",
       "      type    size    city category  unit_cost  \n",
       "0  grocery  medium  Boston  grocery     107.38  \n",
       "1  grocery  medium  Boston  grocery     159.61  \n",
       "2  grocery  medium  Boston  grocery     135.62  \n",
       "3  grocery  medium  Boston  grocery     151.07  \n",
       "4  grocery  medium  Boston  grocery      54.70  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf8d878-ec54-4a4a-81cf-fe27e42f8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping sales by store, product, and date\n",
    "daily_sales = sales_df.groupby(['store_id', 'product_id', 'date']).agg({\n",
    "    'sales_quantity': 'sum',\n",
    "    'total_revenue': 'sum'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8805db7c-5cc5-461f-9382-9fc699eee9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily sales aggregated: 31696 records\n"
     ]
    }
   ],
   "source": [
    "print(f\"Daily sales aggregated: {len(daily_sales)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828b24ac-28ff-4a3d-9e08-877e44491ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by date for time-based features\n",
    "daily_sales = daily_sales.sort_values(['store_id', 'product_id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e13787-7be6-4f86-a693-bdc03c0ff548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create moving averages\n",
    "daily_sales['sales_7day_avg'] = daily_sales.groupby(['store_id', 'product_id'])['sales_quantity'].rolling(7, min_periods=1).mean().values\n",
    "daily_sales['sales_30day_avg'] = daily_sales.groupby(['store_id', 'product_id'])['sales_quantity'].rolling(30, min_periods=1).mean().values\n",
    "\n",
    "\n",
    "#We are calculating moving averages to smooth out noise and capture the underlying trend in sales over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef3fbd8-4195-4de0-80f1-0402dd124445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lag features (previous day sales)\n",
    "daily_sales['sales_lag_1'] = daily_sales.groupby(['store_id', 'product_id'])['sales_quantity'].shift(1)\n",
    "daily_sales['sales_lag_7'] = daily_sales.groupby(['store_id', 'product_id'])['sales_quantity'].shift(7)\n",
    "\n",
    "#we are calculating lag features to give the model direct access to past values so it can learn how past sales influence future sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b95e2-5e08-4546-9bb5-135a30efe714",
   "metadata": {},
   "source": [
    "#### In general we are calculating moving averages and lag features to capture trend over time. which will allow the model to learn from the past behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb384ac-3713-40e5-ba44-d99ab16b23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales trend (is it increasing or decreasing?)\n",
    "daily_sales['sales_trend'] = daily_sales['sales_7day_avg'] - daily_sales.groupby(['store_id', 'product_id'])['sales_7day_avg'].shift(7)\n",
    "\n",
    "#Here we are calculating the change in the 7-day moving average compared to the same day last week for each product in each store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8811ff9-30d2-4cff-b15c-6e72065e4c75",
   "metadata": {},
   "source": [
    "# CREATE DATE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86b87c49-cb7c-453c-9843-7f8917d40629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic date features\n",
    "daily_sales['day_of_week'] = daily_sales['date'].dt.dayofweek  # 0=Monday\n",
    "daily_sales['month'] = daily_sales['date'].dt.month\n",
    "daily_sales['quarter'] = daily_sales['date'].dt.quarter\n",
    "daily_sales['is_weekend'] = (daily_sales['date'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "\n",
    "#To help the model understand when sales are happening because time affects consumer behavior.\n",
    "\n",
    "\n",
    "#Yeah we have to think in a way the model would think 🙂. The model is like baby who doesnt know anything and we need to teach him everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fe366bf-cad2-4ca2-8bfc-b62a8d68c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holiday features\n",
    "# Basically listing the holidays\n",
    "\n",
    "holidays = ['2022-01-01', '2022-07-04', '2022-11-24', '2022-12-25',\n",
    "           '2023-01-01', '2023-07-04', '2023-11-23', '2023-12-25',\n",
    "           '2024-01-01', '2024-07-04', '2024-11-28', '2024-12-25']\n",
    "\n",
    "holiday_dates = pd.to_datetime(holidays)\n",
    "daily_sales['is_holiday'] = daily_sales['date'].isin(holiday_dates).astype(int)\n",
    "\n",
    "#The above creates a binary feature:\n",
    "#1 = the date is a holiday.\n",
    "#0 = the date is not a holiday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb8d02a8-4a9e-4436-9a10-165e93aa970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days until next holiday\n",
    "def days_to_next_holiday(date):\n",
    "    future_holidays = [h for h in holiday_dates if h > date]\n",
    "    if future_holidays:\n",
    "        return (min(future_holidays) - date).days\n",
    "    return 365\n",
    "\n",
    "daily_sales['days_to_holiday'] = daily_sales['date'].apply(days_to_next_holiday)\n",
    "\n",
    "#For each date in your dataset, it calculates the number of days until the next holiday.\n",
    "\n",
    "#If there's no future holiday left in the list, it returns 365 (acts like a fallback value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace436b-69fe-42d0-b722-d67d00365398",
   "metadata": {},
   "source": [
    "# MERGE WITH EXTERNAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d086ee1-03a2-4493-ad6e-07cbec1bff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with external data\n",
    "daily_sales = daily_sales.merge(external_df, on='date', how='left')\n",
    "\n",
    "#Lets merge the daily sales data with external data to provide extra context it couldn’t learn from internal sales data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47253141-4919-4cce-baa1-f1651968d8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreates a binary feature: 1 if it rained that day, 0 if not.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create weather impact features\n",
    "daily_sales['temp_category'] = pd.cut(daily_sales['temperature'], \n",
    "                                     bins=[-50, 32, 70, 100], \n",
    "                                     labels=['cold', 'mild', 'hot'])\n",
    "\n",
    "\"\"\"\n",
    "Temperature affects consumer behavior:\n",
    "\n",
    "Ice cream sells more in hot weather.\n",
    "\n",
    "Hot beverages sell more in cold.\n",
    "\n",
    "Foot traffic may increase in mild temperatures.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "daily_sales['has_rain'] = (daily_sales['precipitation'] > 0).astype(int)\n",
    "\n",
    "\"\"\"\n",
    "Creates a binary feature: 1 if it rained that day, 0 if not.\n",
    "\"\"\"\n",
    "\n",
    "#its like saying .....if it's raining and cold, people probably stay in and hot chocolate sells more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37490eb-917a-43fc-b94a-7a7107d00533",
   "metadata": {},
   "source": [
    "# CREATE INVENTORY FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55c60788-25aa-49e0-aaf6-067bc975ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0/143889\n",
      "Processing row 1000/143889\n",
      "Processing row 2000/143889\n",
      "Processing row 3000/143889\n",
      "Processing row 4000/143889\n",
      "Processing row 5000/143889\n",
      "Processing row 6000/143889\n",
      "Processing row 7000/143889\n",
      "Processing row 8000/143889\n",
      "Processing row 9000/143889\n",
      "Processing row 10000/143889\n",
      "Processing row 11000/143889\n",
      "Processing row 12000/143889\n",
      "Processing row 13000/143889\n",
      "Processing row 14000/143889\n",
      "Processing row 15000/143889\n",
      "Processing row 16000/143889\n",
      "Processing row 17000/143889\n",
      "Processing row 18000/143889\n",
      "Processing row 19000/143889\n",
      "Processing row 20000/143889\n",
      "Processing row 21000/143889\n",
      "Processing row 22000/143889\n",
      "Processing row 23000/143889\n",
      "Processing row 24000/143889\n",
      "Processing row 25000/143889\n",
      "Processing row 26000/143889\n",
      "Processing row 27000/143889\n",
      "Processing row 28000/143889\n",
      "Processing row 29000/143889\n",
      "Processing row 30000/143889\n",
      "Processing row 31000/143889\n",
      "Processing row 32000/143889\n",
      "Processing row 33000/143889\n",
      "Processing row 34000/143889\n",
      "Processing row 35000/143889\n",
      "Processing row 36000/143889\n",
      "Processing row 37000/143889\n",
      "Processing row 38000/143889\n",
      "Processing row 39000/143889\n",
      "Processing row 40000/143889\n",
      "Processing row 41000/143889\n",
      "Processing row 42000/143889\n",
      "Processing row 43000/143889\n",
      "Processing row 44000/143889\n",
      "Processing row 45000/143889\n",
      "Processing row 46000/143889\n",
      "Processing row 47000/143889\n",
      "Processing row 48000/143889\n",
      "Processing row 49000/143889\n",
      "Processing row 50000/143889\n",
      "Processing row 51000/143889\n",
      "Processing row 52000/143889\n",
      "Processing row 53000/143889\n",
      "Processing row 54000/143889\n",
      "Processing row 55000/143889\n",
      "Processing row 56000/143889\n",
      "Processing row 57000/143889\n",
      "Processing row 58000/143889\n",
      "Processing row 59000/143889\n",
      "Processing row 60000/143889\n",
      "Processing row 61000/143889\n",
      "Processing row 62000/143889\n",
      "Processing row 63000/143889\n",
      "Processing row 64000/143889\n",
      "Processing row 65000/143889\n",
      "Processing row 66000/143889\n",
      "Processing row 67000/143889\n",
      "Processing row 68000/143889\n",
      "Processing row 69000/143889\n",
      "Processing row 70000/143889\n",
      "Processing row 71000/143889\n",
      "Processing row 72000/143889\n",
      "Processing row 73000/143889\n",
      "Processing row 74000/143889\n",
      "Processing row 75000/143889\n",
      "Processing row 76000/143889\n",
      "Processing row 77000/143889\n",
      "Processing row 78000/143889\n",
      "Processing row 79000/143889\n",
      "Processing row 80000/143889\n",
      "Processing row 81000/143889\n",
      "Processing row 82000/143889\n",
      "Processing row 83000/143889\n",
      "Processing row 84000/143889\n",
      "Processing row 85000/143889\n",
      "Processing row 86000/143889\n",
      "Processing row 87000/143889\n",
      "Processing row 88000/143889\n",
      "Processing row 89000/143889\n",
      "Processing row 90000/143889\n",
      "Processing row 91000/143889\n",
      "Processing row 92000/143889\n",
      "Processing row 93000/143889\n",
      "Processing row 94000/143889\n",
      "Processing row 95000/143889\n",
      "Processing row 96000/143889\n",
      "Processing row 97000/143889\n",
      "Processing row 98000/143889\n",
      "Processing row 99000/143889\n",
      "Processing row 100000/143889\n",
      "Processing row 101000/143889\n",
      "Processing row 102000/143889\n",
      "Processing row 103000/143889\n",
      "Processing row 104000/143889\n",
      "Processing row 105000/143889\n",
      "Processing row 106000/143889\n",
      "Processing row 107000/143889\n",
      "Processing row 108000/143889\n",
      "Processing row 109000/143889\n",
      "Processing row 110000/143889\n",
      "Processing row 111000/143889\n",
      "Processing row 112000/143889\n",
      "Processing row 113000/143889\n",
      "Processing row 114000/143889\n",
      "Processing row 115000/143889\n",
      "Processing row 116000/143889\n",
      "Processing row 117000/143889\n",
      "Processing row 118000/143889\n",
      "Processing row 119000/143889\n",
      "Processing row 120000/143889\n",
      "Processing row 121000/143889\n",
      "Processing row 122000/143889\n",
      "Processing row 123000/143889\n",
      "Processing row 124000/143889\n",
      "Processing row 125000/143889\n",
      "Processing row 126000/143889\n",
      "Processing row 127000/143889\n",
      "Processing row 128000/143889\n",
      "Processing row 129000/143889\n",
      "Processing row 130000/143889\n",
      "Processing row 131000/143889\n",
      "Processing row 132000/143889\n",
      "Processing row 133000/143889\n",
      "Processing row 134000/143889\n",
      "Processing row 135000/143889\n",
      "Processing row 136000/143889\n",
      "Processing row 137000/143889\n",
      "Processing row 138000/143889\n",
      "Processing row 139000/143889\n",
      "Processing row 140000/143889\n",
      "Processing row 141000/143889\n",
      "Processing row 142000/143889\n",
      "Processing row 143000/143889\n"
     ]
    }
   ],
   "source": [
    "# For each sales record, find the closest inventory snapshot\n",
    "inventory_features = []\n",
    "\n",
    "for idx, row in daily_sales.iterrows():\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"Processing row {idx}/{len(daily_sales)}\")\n",
    "    \n",
    "    # Find inventory data for this store/product around this date\n",
    "    inv_data = inventory_df[\n",
    "        (inventory_df['store_id'] == row['store_id']) & \n",
    "        (inventory_df['product_id'] == row['product_id']) &\n",
    "        (inventory_df['snapshot_date'] <= row['date'])\n",
    "    ]\n",
    "    \n",
    "    if not inv_data.empty:\n",
    "        # Get the most recent inventory snapshot\n",
    "        latest_inv = inv_data.loc[inv_data['snapshot_date'].idxmax()]\n",
    "        \n",
    "        inventory_features.append({\n",
    "            'current_stock': latest_inv['current_stock'],\n",
    "            'reorder_point': latest_inv['reorder_point'],\n",
    "            'supplier_lead_time': latest_inv['supplier_lead_time'],\n",
    "            'days_since_snapshot': (row['date'] - latest_inv['snapshot_date']).days\n",
    "        })\n",
    "    else:\n",
    "        inventory_features.append({\n",
    "            'current_stock': np.nan,\n",
    "            'reorder_point': np.nan, \n",
    "            'supplier_lead_time': np.nan,\n",
    "            'days_since_snapshot': np.nan\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa295ee7-0010-4d7a-a2e8-cae4b2097060",
   "metadata": {},
   "source": [
    "### In the above code we are looping through every row in daily_sales, and for each row:\n",
    "\n",
    "- Finds the matching inventory data (inventory_df) for the same store and product, where the inventory snapshot is on or before the sale date.\n",
    "\n",
    "- From those, it picks the most recent snapshot (idxmax() on snapshot_date).\n",
    "\n",
    "- Extracts key inventory-related values and appends them as a dictionary to inventory_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "665eafce-a95a-4973-8f2f-e2c0330d6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add inventory features to main dataframe\n",
    "inv_features_df = pd.DataFrame(inventory_features)\n",
    "daily_sales = pd.concat([daily_sales, inv_features_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77890671-1006-4c76-bab6-b594ad9591f0",
   "metadata": {},
   "source": [
    "#### The above code merges the engineered inventory features into your main dataset, aligning them with each corresponding sales record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3a2f88a-7623-4ae1-92ec-75e4af8b4774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MUNNA\\AppData\\Local\\Temp\\ipykernel_12596\\639307798.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_sales['current_stock'].fillna(daily_sales['sales_30day_avg'] * 10, inplace=True)\n",
      "C:\\Users\\MUNNA\\AppData\\Local\\Temp\\ipykernel_12596\\639307798.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_sales['reorder_point'].fillna(daily_sales['sales_7day_avg'] * 3, inplace=True)\n",
      "C:\\Users\\MUNNA\\AppData\\Local\\Temp\\ipykernel_12596\\639307798.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_sales['supplier_lead_time'].fillna(7, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill missing inventory values\n",
    "daily_sales['current_stock'].fillna(daily_sales['sales_30day_avg'] * 10, inplace=True)\n",
    "daily_sales['reorder_point'].fillna(daily_sales['sales_7day_avg'] * 3, inplace=True)\n",
    "daily_sales['supplier_lead_time'].fillna(7, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfecab-13c6-490c-b88d-8478e3dcb79f",
   "metadata": {},
   "source": [
    "#### The above code ensures your dataset has no nulls in key inventory features by imputing them based on recent sales trends or a fixed fallback.\n",
    "\n",
    "(also please ignore the warning 😁)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4fc79-c57a-46e5-b1d9-b8cafcf21a43",
   "metadata": {},
   "source": [
    "# CREATE TARGET VARIABLE (RISK CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2416fe36-e715-4614-85a1-8b890e15a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate days of stock remaining\n",
    "daily_sales['days_of_stock'] = daily_sales['current_stock'] / (daily_sales['sales_7day_avg'] + 0.1)  # +0.1 to avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceef5eab-2b99-4190-9abc-b378d4c49d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted demand for next week\n",
    "daily_sales['predicted_demand_7days'] = daily_sales['sales_7day_avg'] * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51971e3c-ac1e-4db5-9097-e1fd7a4b060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk categories based on business logic\n",
    "def calculate_risk_category(row):\n",
    "    days_stock = row['days_of_stock']\n",
    "    current_stock = row['current_stock']\n",
    "    reorder_point = row['reorder_point']\n",
    "    sales_trend = row['sales_trend']\n",
    "    \n",
    "    # High risk situations\n",
    "    if days_stock < 3 or current_stock <= reorder_point:\n",
    "        return 'HIGH_RISK'\n",
    "    \n",
    "    # Stockout risk (low stock with increasing demand)\n",
    "    elif days_stock < 7 and sales_trend > 0:\n",
    "        return 'STOCKOUT_RISK'\n",
    "    \n",
    "    # Overstock risk (too much stock with decreasing demand) \n",
    "    elif days_stock > 30 and sales_trend < -2:\n",
    "        return 'OVERSTOCK_RISK'\n",
    "    \n",
    "    # Normal situation\n",
    "    else:\n",
    "        return 'LOW_RISK'\n",
    "\n",
    "daily_sales['risk_category'] = daily_sales.apply(calculate_risk_category, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912f9ce-ee24-4586-ad0e-b7529642f571",
   "metadata": {},
   "source": [
    "### It creates a strategic feature (risk_category) that flags whether a product-store-date is at risk of stockout, overstock, or operating normally.\n",
    "\n",
    "Business logic captured:\n",
    "- HIGH_RISK: Very low stock or stock already below reorder point.\n",
    "\n",
    "- STOCKOUT_RISK: Low stock and rising demand-danger of running out.\n",
    "\n",
    "- OVERSTOCK_RISK: High stock and falling demand-risk of excess inventory.\n",
    "\n",
    "- LOW_RISK: All other cases-stable inventory situation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acfb1c50-cad8-485b-9462-0ac2c08f257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric risk score (0-100)\n",
    "risk_mapping = {'LOW_RISK': 25, 'STOCKOUT_RISK': 65, 'OVERSTOCK_RISK': 60, 'HIGH_RISK': 90}\n",
    "daily_sales['risk_score'] = daily_sales['risk_category'].map(risk_mapping)\n",
    "\n",
    "\n",
    "# It transforms the categorical risk_category into a quantitative risk_score that reflects the severity of inventory risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc275462-9249-43c3-b912-f8a434fd8d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable created!\n",
      "\n",
      "Risk Category Distribution:\n",
      "risk_category\n",
      "LOW_RISK          74649\n",
      "HIGH_RISK         60072\n",
      "OVERSTOCK_RISK     7053\n",
      "STOCKOUT_RISK      2115\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Target variable created!\")\n",
    "print(\"\\nRisk Category Distribution:\")\n",
    "print(daily_sales['risk_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a725c5-5b53-4d76-b0db-9e850fd0c1e5",
   "metadata": {},
   "source": [
    "# FINAL FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4972828d-e432-464f-a8b9-ba284631bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    # Sales features\n",
    "    'sales_7day_avg', 'sales_30day_avg', 'sales_lag_1', 'sales_lag_7', 'sales_trend',\n",
    "    \n",
    "    # Date features  \n",
    "    'day_of_week', 'month', 'quarter', 'is_weekend', 'is_holiday', 'days_to_holiday',\n",
    "    \n",
    "    # External features\n",
    "    'temperature', 'precipitation', 'has_rain', 'competitor_promotion', 'local_event',\n",
    "    \n",
    "    # Inventory features\n",
    "    'current_stock', 'reorder_point', 'supplier_lead_time', 'days_of_stock',\n",
    "    \n",
    "    # Target\n",
    "    'risk_category', 'risk_score'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e546098c-d265-4378-b144-9e71e8322381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep ID columns\n",
    "id_columns = ['store_id', 'product_id', 'date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14f30012-4e99-4adc-a80c-015befe77990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset\n",
    "final_features = daily_sales[id_columns + feature_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93140b99-e039-408b-aac9-f37357faab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle remaining missing values\n",
    "final_features = final_features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a9bf975-582d-4ef2-9dc8-b530cb5e8773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (143889, 25)\n",
      "Features selected: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final dataset shape: {final_features.shape}\")\n",
    "print(f\"Features selected: {len(feature_columns)-2}\")  # -2 for target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d3d7b-92d5-46c6-98c2-528c2bafc4f5",
   "metadata": {},
   "source": [
    "# SAVE PROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2c9d0fa-b5f7-4b74-aa92-944da4bb2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the engineered features\n",
    "final_features.to_csv('data_processed/features_engineered.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4976f105-b466-4307-993a-c03df173f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature list for later use\n",
    "feature_list = [col for col in feature_columns if col not in ['risk_category', 'risk_score']]\n",
    "pd.DataFrame({'feature_name': feature_list}).to_csv('data_processed/feature_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7e70a-c953-4ffa-b216-1c6ce67020dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
